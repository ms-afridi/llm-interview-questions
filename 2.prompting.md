# Prompt Engineering - Interview Questions & Answers

## Table of Contents
1. [Basic Prompting Concepts](#basic-prompting-concepts)
2. [Advanced Techniques](#advanced-techniques)
3. [Few-Shot & Chain-of-Thought](#few-shot--chain-of-thought)
4. [Prompt Optimization](#prompt-optimization)
5. [Best Practices](#best-practices)

---

## Basic Prompting Concepts

### Q1: What is prompt engineering? Why is it important?

**Answer:**

Prompt engineering is the practice of designing and optimizing input prompts to get desired outputs from LLMs. It's like learning to ask the right questions to get the right answers.

**Why It's Important:**

```
Same model, different prompts:

Bad prompt: "code"
Output: "Sure, I can help with code..."

Good prompt: "Write a Python function to reverse a string"
Output: def reverse_string(s): return s[::-1]

Same GPT-4, 100x better result!
```

**Key Benefits:**

1. **No Training Required:** Improve performance without fine-tuning
2. **Cost-Effective:** Free performance boost
3. **Fast Iteration:** Test new approaches in seconds
4. **Flexibility:** Same model, many tasks

**Real Impact:**

```
Customer Support Bot:

Generic prompt:
"You are a helpful assistant"
→ Accuracy: 60%

Engineered prompt:
"You are a customer support agent for TechCorp.
Rules: 1) Be polite 2) Provide step-by-step solutions
3) Escalate if you can't help 4) Never share personal data"
→ Accuracy: 85%

No model change, just better prompting!
```

**Interview Insight:** Good prompt engineering can make a cheaper model (GPT-3.5) perform like an expensive one (GPT-4) for specific tasks!

---

### Q2: What are the key components of a good prompt?

**Answer:**

A well-structured prompt has four key components:

**1. Instruction (What to do):**
```
"Translate the following text to French"
"Summarize this article in 3 bullet points"
"Extract all email addresses from the text"
```

**2. Context (Background information):**
```
"You are a medical expert specializing in cardiology"
"This is a formal business email"
"The user is a beginner programmer"
```

**3. Input Data (What to process):**
```
"Text: [article content]"
"Email: Dear Sir, ..."
"Code: def hello()..."
```

**4. Output Format (How to respond):**
```
"Output as JSON with keys: name, age, location"
"Respond in bullet points"
"Format: Question | Answer | Confidence (1-10)"
```

**Complete Example:**

```
[Context]
You are an experienced Python developer reviewing code.

[Instruction]
Review the following code for bugs and suggest improvements.

[Input]
def calculate_average(numbers):
    return sum(numbers) / len(numbers)

[Output Format]
Provide:
1. Bugs found (if any)
2. Security issues
3. Suggested improvements
4. Refactored code
```

**Simple vs. Structured:**

```
❌ Simple (vague):
"Tell me about Python"

✅ Structured (clear):
Context: "I'm learning web development"
Instruction: "Explain Python's role in web development"
Output: "Give 3 main use cases with examples"
```

**Template Pattern:**

```python
prompt = f"""
You are a {role}.

Task: {instruction}

Input: {input_data}

Requirements:
- {requirement_1}
- {requirement_2}

Output format: {format}
"""
```

**Key Insight:** The more specific your prompt, the better the output. Ambiguity = unpredictable results!

---

### Q3: What is the difference between zero-shot, one-shot, and few-shot prompting?

**Answer:**

These differ in how many examples you provide to guide the model.

**Zero-Shot:**
No examples, just the instruction.

```
Prompt: "Classify the sentiment: 'This movie was amazing!'"
Output: "Positive"

When to use:
- Simple, well-known tasks
- Model already knows the task
- Quick experiments
```

**One-Shot:**
Provide one example.

```
Prompt:
"Classify sentiment:
Example: 'Great service!' → Positive

Now classify: 'Terrible experience!'"

Output: "Negative"

When to use:
- Clarify output format
- Show desired style
- Novel task structures
```

**Few-Shot:**
Provide 2-10 examples.

```
Prompt:
"Classify sentiment:
'Excellent product!' → Positive
'Waste of money' → Negative  
'It's okay' → Neutral

Classify: 'Best purchase ever!'"

Output: "Positive"

When to use:
- Complex patterns
- Improve accuracy
- Ambiguous tasks
```

**Performance Comparison:**

```
Task: Custom entity extraction

Zero-shot: 65% accuracy
One-shot: 75% accuracy
Few-shot (5 examples): 88% accuracy

Diminishing returns after 5-10 examples
```

**Real Example - Custom Classification:**

```
❌ Zero-shot (confused):
"Classify: 'Need refund ASAP!'"
Output: "Urgent" (but we wanted category: billing, not urgency)

✅ Few-shot (clear):
"Classify customer queries:
'Login not working' → Technical
'Want my money back' → Billing
'How do I upgrade?' → Sales

Classify: 'Need refund ASAP!'"
Output: "Billing" ✓
```

**Best Practices:**

```
1. Start with zero-shot (fastest)
2. Add one example if format is unclear
3. Use few-shot (3-5 examples) if:
   - Accuracy too low
   - Task is novel
   - Edge cases need coverage

4. Don't exceed 10 examples:
   - Wastes tokens
   - Diminishing returns
   - Consider fine-tuning instead
```

---

## Advanced Techniques

### Q4: What is Chain-of-Thought (CoT) prompting? When should you use it?

**Answer:**

Chain-of-Thought prompting encourages the model to show its reasoning steps before giving the final answer. Like showing your work in math class.

**How It Works:**

**Without CoT:**
```
Q: "A store has 15 apples. They sell 6 and receive 8 more. How many now?"
A: "17" 
(might be right, but we don't know how it got there)
```

**With CoT:**
```
Q: "A store has 15 apples. They sell 6 and receive 8 more. How many now?
Let's think step by step:"

A: "Let me work through this:
1. Starting amount: 15 apples
2. After selling 6: 15 - 6 = 9 apples
3. After receiving 8: 9 + 8 = 17 apples
Answer: 17 apples"
```

**Two Main Approaches:**

**1. Manual CoT (Few-shot with reasoning):**

```
Prompt:
Q: "Roger has 5 tennis balls. He buys 2 more. How many does he have?"
A: "Roger starts with 5 balls. He buys 2 more: 5 + 2 = 7. Answer: 7"

Q: "A cafe has 12 chairs. 5 break. They buy 8 new ones. How many now?"
A: "Let me calculate:
- Start: 12 chairs
- After 5 break: 12 - 5 = 7 chairs  
- After buying 8: 7 + 8 = 15 chairs
Answer: 15"

Now you try:
Q: "Sarah has 10 cookies. She gives 3 to John and 2 to Mary. How many left?"
```

**2. Zero-shot CoT (Magic phrase):**

```
Just add: "Let's think step by step"

Prompt: "What is 25% of 80? Let's think step by step."

Output:
"To find 25% of 80:
1. Convert 25% to decimal: 25/100 = 0.25
2. Multiply: 80 × 0.25 = 20
Answer: 20"
```

**When to Use CoT:**

```
✅ Use for:
- Math problems
- Logic puzzles
- Multi-hop reasoning
- Complex decision-making
- Debugging (step through logic)

❌ Skip for:
- Simple lookups ("What is the capital of France?")
- Classification ("Sentiment: positive/negative")
- Quick facts
- When you need fast responses
```

**Performance Impact:**

```
Math Word Problems (GSM8K):

Standard prompting: 17% accuracy
CoT prompting: 58% accuracy

3x improvement just from "Let's think step by step"!
```

**Real Example - Complex Reasoning:**

```
Problem: "John is faster than Mary. Mary is faster than Tom. 
Who is slowest?"

Without CoT:
A: "Tom" ✓ (might guess right)

With CoT:
A: "Let's analyze the relationships:
1. John > Mary (John is faster)
2. Mary > Tom (Mary is faster)
3. Therefore: John > Mary > Tom
The slowest is at the end of the chain.
Answer: Tom" ✓ (reasoning is clear)
```

**Advanced: Self-Consistency:**

```
Generate multiple CoT reasoning paths:

Path 1: "15 - 6 = 9, 9 + 8 = 17"
Path 2: "Start 15, after -6 and +8: 15 - 6 + 8 = 17"  
Path 3: "Subtract 6: 15-6=9. Add 8: 9+8=17"

Most common answer: 17 ✓ (high confidence!)
```

**Key Insight:** CoT trades tokens (longer prompts/responses) for accuracy. Use when accuracy matters more than speed!

---
