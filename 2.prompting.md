# Prompt Engineering - Interview Questions & Answers

## Table of Contents
1. [Basic Prompting Concepts](#basic-prompting-concepts)
2. [Advanced Techniques](#advanced-techniques)
3. [Few-Shot & Chain-of-Thought](#few-shot--chain-of-thought)
4. [Prompt Optimization](#prompt-optimization)
5. [Best Practices](#best-practices)

---

## Basic Prompting Concepts

### Q1: What is prompt engineering? Why is it important?

**Answer:**

Prompt engineering is the practice of designing and optimizing input prompts to get desired outputs from LLMs. It's like learning to ask the right questions to get the right answers.

**Why It's Important:**

```
Same model, different prompts:

Bad prompt: "code"
Output: "Sure, I can help with code..."

Good prompt: "Write a Python function to reverse a string"
Output: def reverse_string(s): return s[::-1]

Same GPT-4, 100x better result!
```

**Key Benefits:**

1. **No Training Required:** Improve performance without fine-tuning
2. **Cost-Effective:** Free performance boost
3. **Fast Iteration:** Test new approaches in seconds
4. **Flexibility:** Same model, many tasks

**Real Impact:**

```
Customer Support Bot:

Generic prompt:
"You are a helpful assistant"
â†’ Accuracy: 60%

Engineered prompt:
"You are a customer support agent for TechCorp.
Rules: 1) Be polite 2) Provide step-by-step solutions
3) Escalate if you can't help 4) Never share personal data"
â†’ Accuracy: 85%

No model change, just better prompting!
```

**Interview Insight:** Good prompt engineering can make a cheaper model (GPT-3.5) perform like an expensive one (GPT-4) for specific tasks!

---

### Q2: What are the key components of a good prompt?

**Answer:**

A well-structured prompt has four key components:

**1. Instruction (What to do):**
```
"Translate the following text to French"
"Summarize this article in 3 bullet points"
"Extract all email addresses from the text"
```

**2. Context (Background information):**
```
"You are a medical expert specializing in cardiology"
"This is a formal business email"
"The user is a beginner programmer"
```

**3. Input Data (What to process):**
```
"Text: [article content]"
"Email: Dear Sir, ..."
"Code: def hello()..."
```

**4. Output Format (How to respond):**
```
"Output as JSON with keys: name, age, location"
"Respond in bullet points"
"Format: Question | Answer | Confidence (1-10)"
```

**Complete Example:**

```
[Context]
You are an experienced Python developer reviewing code.

[Instruction]
Review the following code for bugs and suggest improvements.

[Input]
def calculate_average(numbers):
    return sum(numbers) / len(numbers)

[Output Format]
Provide:
1. Bugs found (if any)
2. Security issues
3. Suggested improvements
4. Refactored code
```

**Simple vs. Structured:**

```
âŒ Simple (vague):
"Tell me about Python"

âœ… Structured (clear):
Context: "I'm learning web development"
Instruction: "Explain Python's role in web development"
Output: "Give 3 main use cases with examples"
```

**Template Pattern:**

```python
prompt = f"""
You are a {role}.

Task: {instruction}

Input: {input_data}

Requirements:
- {requirement_1}
- {requirement_2}

Output format: {format}
"""
```

**Key Insight:** The more specific your prompt, the better the output. Ambiguity = unpredictable results!

---

### Q3: What is the difference between zero-shot, one-shot, and few-shot prompting?

**Answer:**

These differ in how many examples you provide to guide the model.

**Zero-Shot:**
No examples, just the instruction.

```
Prompt: "Classify the sentiment: 'This movie was amazing!'"
Output: "Positive"

When to use:
- Simple, well-known tasks
- Model already knows the task
- Quick experiments
```

**One-Shot:**
Provide one example.

```
Prompt:
"Classify sentiment:
Example: 'Great service!' â†’ Positive

Now classify: 'Terrible experience!'"

Output: "Negative"

When to use:
- Clarify output format
- Show desired style
- Novel task structures
```

**Few-Shot:**
Provide 2-10 examples.

```
Prompt:
"Classify sentiment:
'Excellent product!' â†’ Positive
'Waste of money' â†’ Negative  
'It's okay' â†’ Neutral

Classify: 'Best purchase ever!'"

Output: "Positive"

When to use:
- Complex patterns
- Improve accuracy
- Ambiguous tasks
```

**Performance Comparison:**

```
Task: Custom entity extraction

Zero-shot: 65% accuracy
One-shot: 75% accuracy
Few-shot (5 examples): 88% accuracy

Diminishing returns after 5-10 examples
```

**Real Example - Custom Classification:**

```
âŒ Zero-shot (confused):
"Classify: 'Need refund ASAP!'"
Output: "Urgent" (but we wanted category: billing, not urgency)

âœ… Few-shot (clear):
"Classify customer queries:
'Login not working' â†’ Technical
'Want my money back' â†’ Billing
'How do I upgrade?' â†’ Sales

Classify: 'Need refund ASAP!'"
Output: "Billing" âœ“
```

**Best Practices:**

```
1. Start with zero-shot (fastest)
2. Add one example if format is unclear
3. Use few-shot (3-5 examples) if:
   - Accuracy too low
   - Task is novel
   - Edge cases need coverage

4. Don't exceed 10 examples:
   - Wastes tokens
   - Diminishing returns
   - Consider fine-tuning instead
```

---

## Advanced Techniques

### Q4: What is Chain-of-Thought (CoT) prompting? When should you use it?

**Answer:**

Chain-of-Thought prompting encourages the model to show its reasoning steps before giving the final answer. Like showing your work in math class.

**How It Works:**

**Without CoT:**
```
Q: "A store has 15 apples. They sell 6 and receive 8 more. How many now?"
A: "17" 
(might be right, but we don't know how it got there)
```

**With CoT:**
```
Q: "A store has 15 apples. They sell 6 and receive 8 more. How many now?
Let's think step by step:"

A: "Let me work through this:
1. Starting amount: 15 apples
2. After selling 6: 15 - 6 = 9 apples
3. After receiving 8: 9 + 8 = 17 apples
Answer: 17 apples"
```

**Two Main Approaches:**

**1. Manual CoT (Few-shot with reasoning):**

```
Prompt:
Q: "Roger has 5 tennis balls. He buys 2 more. How many does he have?"
A: "Roger starts with 5 balls. He buys 2 more: 5 + 2 = 7. Answer: 7"

Q: "A cafe has 12 chairs. 5 break. They buy 8 new ones. How many now?"
A: "Let me calculate:
- Start: 12 chairs
- After 5 break: 12 - 5 = 7 chairs  
- After buying 8: 7 + 8 = 15 chairs
Answer: 15"

Now you try:
Q: "Sarah has 10 cookies. She gives 3 to John and 2 to Mary. How many left?"
```

**2. Zero-shot CoT (Magic phrase):**

```
Just add: "Let's think step by step"

Prompt: "What is 25% of 80? Let's think step by step."

Output:
"To find 25% of 80:
1. Convert 25% to decimal: 25/100 = 0.25
2. Multiply: 80 Ã— 0.25 = 20
Answer: 20"
```

**When to Use CoT:**

```
âœ… Use for:
- Math problems
- Logic puzzles
- Multi-hop reasoning
- Complex decision-making
- Debugging (step through logic)

âŒ Skip for:
- Simple lookups ("What is the capital of France?")
- Classification ("Sentiment: positive/negative")
- Quick facts
- When you need fast responses
```

**Performance Impact:**

```
Math Word Problems (GSM8K):

Standard prompting: 17% accuracy
CoT prompting: 58% accuracy

3x improvement just from "Let's think step by step"!
```

**Real Example - Complex Reasoning:**

```
Problem: "John is faster than Mary. Mary is faster than Tom. 
Who is slowest?"

Without CoT:
A: "Tom" âœ“ (might guess right)

With CoT:
A: "Let's analyze the relationships:
1. John > Mary (John is faster)
2. Mary > Tom (Mary is faster)
3. Therefore: John > Mary > Tom
The slowest is at the end of the chain.
Answer: Tom" âœ“ (reasoning is clear)
```

**Advanced: Self-Consistency:**

```
Generate multiple CoT reasoning paths:

Path 1: "15 - 6 = 9, 9 + 8 = 17"
Path 2: "Start 15, after -6 and +8: 15 - 6 + 8 = 17"  
Path 3: "Subtract 6: 15-6=9. Add 8: 9+8=17"

Most common answer: 17 âœ“ (high confidence!)
```

**Key Insight:** CoT trades tokens (longer prompts/responses) for accuracy. Use when accuracy matters more than speed!

---

### Q5: What is prompt chaining? How does it differ from a single long prompt?

**Answer:**

Prompt chaining breaks complex tasks into multiple sequential prompts, where each output feeds into the next prompt. Like an assembly line vs. doing everything at once.

**Single Long Prompt:**
```
"Read this article, extract key points, analyze sentiment, 
summarize in French, and create social media posts"

Problems:
- Too complex (model gets confused)
- Hard to debug (where did it fail?)
- All-or-nothing (can't reuse parts)
```

**Prompt Chaining:**
```
Step 1: "Extract key points from: [article]"
â†’ Output: [key_points]

Step 2: "Analyze sentiment of: [key_points]"
â†’ Output: [sentiment]

Step 3: "Summarize in French: [key_points]"
â†’ Output: [french_summary]

Step 4: "Create 3 tweets from: [french_summary]"
â†’ Output: [tweets]

Benefits:
- Each step is simple
- Easy to debug
- Can cache/reuse intermediate results
```

**Real Example - Content Pipeline:**

```python
# Chain: Article â†’ Summary â†’ Translation â†’ Social Posts

# Step 1: Summarization
prompt_1 = f"Summarize in 3 sentences: {article}"
summary = llm(prompt_1)

# Step 2: Translation  
prompt_2 = f"Translate to Spanish: {summary}"
spanish = llm(prompt_2)

# Step 3: Social media
prompt_3 = f"Create 3 Twitter posts from: {spanish}"
tweets = llm(prompt_3)
```

**When to Use Chaining:**

```
âœ… Use chaining when:
- Task has clear sequential steps
- Need to debug intermediate outputs
- Want to reuse parts (cache summaries)
- Single prompt exceeds context window
- Different steps need different models

âŒ Single prompt when:
- Task is simple
- Need faster response (fewer API calls)
- Steps are tightly coupled
```

**Comparison:**

```
Task: Research report from 5 articles

Single Prompt Approach:
1 call, 10,000 tokens â†’ 1 output
Time: 30 seconds
Cost: $0.30
Debug: Hard

Chaining Approach:
5 calls (extract) + 1 call (combine) = 6 calls
Time: 45 seconds  
Cost: $0.35
Debug: Easy (see each step)
Flexibility: High (can retry failed steps)
```

**Advanced: Dynamic Chaining:**

```python
# Conditional chains based on outputs

result = extract_entities(text)

if result['needs_clarification']:
    result = ask_clarifying_questions()
    result = extract_entities(result['answers'])

if result['entity_type'] == 'person':
    bio = get_biography(result['entity_name'])
else:
    info = get_company_info(result['entity_name'])
```

**Error Handling:**

```
Advantage of chaining:

Step 1: Extract â†’ Success âœ“
Step 2: Translate â†’ Failed âŒ
â†’ Retry only Step 2

With single prompt:
One failure â†’ Retry entire thing (expensive!)
```

**Practical Pattern:**

```
Common chain: RAG + Generation

Step 1: "Extract query intent: [user_question]"
Step 2: "Search database for: [intent]" 
Step 3: "Generate answer using: [search_results]"

Each step specialized, easier to optimize!
```

**Interview Insight:** Chaining is production best practice - more control, better debugging, easier optimization!

---

### Q6: Explain role prompting. Why is it effective?

**Answer:**

Role prompting tells the model to act as a specific persona or expert. It sets context for how the model should "think" and respond.

**Basic Pattern:**

```
"You are a [ROLE]. [Task instructions]"
```

**Why It Works:**

LLMs are trained on internet text where experts write differently than beginners. By specifying a role, you activate the relevant training patterns.

```
Without role:
Q: "How do I fix a leaky faucet?"
A: "You could try tightening it or maybe call someone?"

With role:
"You are an experienced plumber."
Q: "How do I fix a leaky faucet?"
A: "First, turn off the water supply. Then check if it's the O-ring 
or washer. Here's the step-by-step repair process..."
```

**Common Role Examples:**

**1. Expert Roles:**
```
"You are a senior Python developer with 10 years experience"
"You are a certified nutritionist"
"You are a Harvard professor of physics"

Effect: More authoritative, detailed, technical
```

**2. Personality Roles:**
```
"You are a friendly elementary school teacher"
"You are a concise technical writer"
"You are an enthusiastic motivational coach"

Effect: Changes tone, complexity, style
```

**3. Functional Roles:**
```
"You are a helpful customer support agent"
"You are a critical code reviewer"
"You are a creative brainstorming partner"

Effect: Shapes behavior and priorities
```

**Real Impact:**

```
Code Review Task:

No role:
"This code looks fine"

Role: "You are a senior security engineer"
"This code has a SQL injection vulnerability on line 12. 
The user input isn't sanitized. Use parameterized queries instead.
Also, the password is stored in plaintext..."

Much more thorough!
```

**Advanced: Multi-Role Prompting:**

```
"You are both a creative writer AND a harsh critic.

First, as a writer, draft a blog post about AI.
Then, as a critic, review it and suggest improvements.
Finally, as the writer, revise based on the critique."

Output:
[Creative draft]
[Critical review]
[Improved version]
```

**Role + Constraints:**

```
"You are a pediatrician explaining to a 5-year-old's parent.

Rules:
- Use simple language (no medical jargon)
- Be reassuring
- Provide actionable steps
- Mention when to seek urgent care"

Effect: Expert knowledge + appropriate communication style
```

**Comparison:**

```
Question: "Explain quantum computing"

No role:
â†’ Generic explanation

"You are a quantum physicist":
â†’ Technical, detailed, assumes background knowledge

"You are a science teacher for high schoolers":
â†’ Simplified, uses analogies, engaging

"You are a tech journalist":
â†’ Focuses on practical applications, current news
```

**Best Practices:**

```
âœ… Do:
- Be specific ("senior DevOps engineer" > "tech person")
- Match role to task complexity
- Combine role with instructions
- Test different roles for your use case

âŒ Don't:
- Use conflicting roles ("expert beginner")
- Expect roles to override model capabilities
- Use only role without clear task
```

**Interview Scenario:**

*Q: "How would you improve response quality for a medical Q&A chatbot?"*

*Good Answer:*
"I'd use role prompting:

```
You are a licensed physician with 15 years of clinical experience.

Guidelines:
- Provide evidence-based information
- Explain medical terms in simple language
- Always include disclaimer: 'This is not a substitute for professional medical advice'
- Suggest seeing a doctor for serious symptoms
- Be empathetic and non-judgmental

Question: {user_question}
```

This activates medical knowledge while ensuring appropriate, safe responses."

**Key Insight:** Roles are free context - use them to guide model behavior without additional training!

---

## Few-Shot & Chain-of-Thought

### Q7: How do you select good examples for few-shot prompting?

**Answer:**

Good few-shot examples are crucial for performance. Quality matters more than quantity.

**Selection Criteria:**

**1. Diversity (Cover different cases):**

```
Bad (all similar):
"Happy customer" â†’ Positive
"Satisfied user" â†’ Positive  
"Pleased buyer" â†’ Positive

Good (diverse):
"Happy customer" â†’ Positive
"Terrible service!" â†’ Negative
"It's okay" â†’ Neutral
"Not bad for the price" â†’ Positive (subtle)
```

**2. Clarity (Unambiguous examples):**

```
Bad:
"It's interesting" â†’ ??? (Unclear sentiment)

Good:
"This is absolutely amazing!" â†’ Positive (Clear)
```

**3. Relevance (Similar to target task):**

```
If classifying customer support tickets:

Bad examples: Movie reviews
Good examples: Customer support messages

Match domain, style, complexity
```

**4. Edge Cases (Include tricky examples):**

```
Standard cases + edge cases:

"Great!" â†’ Positive (standard)
"Not bad" â†’ Positive (edge: double negative)
"It's good but overpriced" â†’ Mixed
"ðŸ˜ŠðŸ‘" â†’ Positive (edge: emojis only)
```

**Optimal Number:**

```
2-3 examples: Minimum for pattern recognition
5-7 examples: Sweet spot for most tasks
10+ examples: Diminishing returns (consider fine-tuning)

Context window limit: Don't exceed 30% of context
```

**Example Selection Strategies:**

**1. Manual Curation:**
```python
# Hand-pick representative examples
examples = [
    {"input": "Love it!", "output": "Positive"},
    {"input": "Waste of money", "output": "Negative"},
    {"input": "Meh", "output": "Neutral"}
]
```

**2. Random Sampling:**
```python
# Random sample from labeled dataset
examples = labeled_data.sample(n=5)
```

**3. Diversity-Based Sampling:**
```python
# Select examples that are most different from each other
# Use embedding similarity
examples = select_diverse_examples(labeled_data, n=5)
```

**4. Dynamic/Semantic Selection:**
```python
# For each query, select most similar examples
query = "This product is amazing but expensive"
similar_examples = find_similar(query, labeled_data, n=3)
# More relevant to specific input
```

**Real Example - Named Entity Recognition:**

```
âŒ Poor selection (all person names):
"John Smith works here" â†’ PERSON: John Smith
"Mary Johnson arrived" â†’ PERSON: Mary Johnson
"Bob Williams left" â†’ PERSON: Bob Williams

âœ… Good selection (diverse entities):
"John Smith works at Microsoft" 
â†’ PERSON: John Smith, ORG: Microsoft

"Meeting on January 15th in Paris"
â†’ DATE: January 15th, LOCATION: Paris

"Apple released iPhone"
â†’ ORG: Apple, PRODUCT: iPhone

Covers all entity types!
```

**Formatting Consistency:**

```
âœ… Consistent format:
Input: "text here" | Output: label
Input: "text here" | Output: label

âŒ Inconsistent format:
"text here" â†’ label
Another input: label  
Text: "here" Output is: label

Keep structure identical!
```

**Quality Check:**

```
Before deploying examples:

1. Test zero-shot first (baseline)
2. Add examples one by one
3. Measure improvement
4. Remove examples that don't help
5. A/B test different example sets
```

**Interview Tip:**

*Q: "You have 1000 labeled examples. How do you select 5 for few-shot prompting?"*

*Strong Answer:*
"I'd use a multi-step approach:

1. **Cluster examples** by similarity (k-means on embeddings)
2. **Select from different clusters** (ensures diversity)
3. **Include edge cases** (hard examples model struggles with)
4. **Validate selection:**
   - Test on held-out set
   - Compare different selections
   - Measure accuracy improvement

5. **Consider dynamic selection:**
   - For each query, select most relevant examples
   - Use semantic similarity (cosine distance)
   - Adapt examples to input

This balances diversity (clusters) with relevance (similarity)!"

**Key Takeaway:** 5 well-chosen examples > 20 random examples. Quality beats quantity!

---

### Q8: What are self-consistency and self-refinement techniques?

**Answer:**

Both techniques improve output quality by having the model generate or improve multiple times.

**Self-Consistency:**

Generate multiple responses and pick the most common answer (majority voting).

**How It Works:**

```
Question: "If a train travels 60 mph for 2.5 hours, how far does it go?"

Generate 5 responses with Chain-of-Thought:

Response 1: "60 Ã— 2.5 = 150 miles" âœ“
Response 2: "60 mph for 2.5 hours = 150 miles" âœ“
Response 3: "Distance = speed Ã— time = 60 Ã— 2.5 = 150 miles" âœ“
Response 4: "2.5 hours at 60 mph = 150 miles" âœ“
Response 5: "60 + 60 + 30 = 150 miles" âœ“

Majority vote: 150 miles (5/5 agree) â†’ High confidence!
```

**With Errors:**

```
Same question, different sampling (temperature=0.8):

Response 1: "150 miles" âœ“
Response 2: "150 miles" âœ“
Response 3: "125 miles" âœ— (wrong calculation)
Response 4: "150 miles" âœ“
Response 5: "150 miles" âœ“

Majority: 150 miles (4/5) â†’ Pick this answer
```

**Implementation:**

```python
def self_consistency(question, n=5):
    answers = []
    
    for i in range(n):
        prompt = f"{question}\nLet's think step by step:"
        response = llm(prompt, temperature=0.7)
        answer = extract_final_answer(response)
        answers.append(answer)
    
    # Return most common answer
    return most_common(answers)
```

**When to Use:**
```
âœ… Use for:
- Math problems
- Logical reasoning
- Multiple valid approaches
- High-stakes decisions

Cost: 5x API calls, but more reliable
```

---

**Self-Refinement:**

Model critiques and improves its own output iteratively.

**How It Works:**

```
Step 1 - Initial Generation:
"Write a professional email requesting a meeting"

Output 1:
"Hey, can we meet sometime? Let me know. Thanks."

Step 2 - Self-Critique:
"Review the above email. Is it professional? What can be improved?"

Critique:
"Too casual. 'Hey' is informal. No specific time. No clear purpose."

Step 3 - Refinement:
"Revise the email based on the critique"

Output 2:
"Dear [Name],
I would like to schedule a meeting to discuss [topic]. 
Would Tuesday at 2 PM work for you?
Best regards,"

Much better!
```

**Iterative Pattern:**

```python
def self_refine(task, iterations=3):
    # Initial generation
    output = llm(f"Task: {task}")
    
    for i in range(iterations):
        # Critique
        critique = llm(f"""
        Review this output: {output}
        What are the issues? How can it be improved?
        """)
        
        # Refine
        output = llm(f"""
        Original: {output}
        Critique: {critique}
        Produce an improved version.
        """)
    
    return output
```

**Real Example - Code Refinement:**

```
Iteration 1:
Code:
def sort_list(lst):
    for i in range(len(lst)):
        for j in range(len(lst)):
            if lst[i] < lst[j]:
                lst[i], lst[j] = lst[j], lst[i]

Critique:
"O(nÂ²) complexity. Inner loop should start at i+1. 
Use Python's built-in sort for simplicity."

Iteration 2:
Code:
def sort_list(lst):
    return sorted(lst)

Better! âœ“
```

**Comparison:**

```
Self-Consistency:
- Generates multiple times in parallel
- Voting/aggregation
- Good for: Finding correct answer among variations
- Cost: NÃ— API calls

Self-Refinement:
- Generates sequentially (improve each time)
- Iterative improvement
- Good for: Creative tasks, code, writing
- Cost: NÃ— API calls but outputs get better
```

**When to Use:**

```
Self-Consistency â†’ Math, logic, factual QA (need correctness)
Self-Refinement â†’ Writing, code, creative (need quality)
Both â†’ High-stakes applications (best of both)
```

---

### Q9: What is the difference between system prompts and user prompts?

**Answer:**

**System Prompt:** Sets overall behavior, role, and constraints. Set once at conversation start.

**User Prompt:** Specific task/query from the user. Changes each interaction.

**Structure:**

```
System: "You are a Python tutor. Be encouraging and explain step-by-step."
User: "How do I reverse a list?"
Assistant: [responds according to system instructions]

User: "What about sorting?"
Assistant: [still follows system instructions]
```

**Key Differences:**

```
Aspect          System Prompt              User Prompt
------          -------------              -----------
Frequency       Once per conversation      Every message
Purpose         Set behavior/rules         Specific task
Visibility      Hidden from user           User writes it
Priority        Higher (stronger)          Lower
Changes         Rarely                     Constantly
```

**Real Example:**

```
System Prompt:
"You are a customer support bot for TechCorp.
Rules:
- Always be polite
- Don't share personal data
- Escalate refund requests to humans
- Provide ticket numbers for all issues"

User Prompt 1: "My order hasn't arrived"
User Prompt 2: "I want a refund"
User Prompt 3: "How do I reset password?"

System prompt applies to ALL responses
```

**Best Practices:**

```
System Prompt (put here):
âœ“ Role definition
âœ“ Tone/style guidelines
âœ“ Safety constraints
âœ“ Output format
âœ“ Domain knowledge

User Prompt (put here):
âœ“ Specific query
âœ“ Input data
âœ“ Task details
```

**Interview Tip:** System prompts are like "configuration", user prompts are like "input data". System sets the rules of the game, user plays the game.

---

### Q10: How do you handle hallucinations through prompt engineering?

**Answer:**

Hallucinations are when LLMs generate false or fabricated information confidently. Prompting can reduce them.

**Techniques:**

**1. Ask for Citations/Sources:**

```
âŒ Generic:
"Tell me about quantum computing"
â†’ Might make up facts

âœ… With citations:
"Explain quantum computing. Cite sources for any factual claims."
â†’ Forces model to be careful or say "I don't have a source"
```

**2. Instruct to Say "I Don't Know":**

```
"If you're not certain, say 'I don't know' rather than guessing.

Question: What is the capital of Atlantis?"
â†’ "I don't know - Atlantis is a mythical place"

vs just answering with made-up information
```

**3. Request Confidence Levels:**

```
"Answer the question and provide confidence (0-100%):

Q: When did Apple release iPhone 15?"
A: "September 2023 (Confidence: 95%)"

Q: "When will iPhone 20 release?"
A: "I cannot predict future releases (Confidence: 0%)"
```

**4. Use Retrieval (RAG Pattern):**

```
"Based ONLY on the following context, answer the question.
If the answer isn't in the context, say 'Not found in context'.

Context: [relevant documents]
Question: [query]"

Restricts model to provided facts
```

**5. Step-by-Step Verification:**

```
"Answer this question, then:
1. List the facts you used
2. Rate your confidence in each fact
3. Provide final answer

Q: What is the population of Mars?"
â†’ Forces model to think about factual basis
```

**6. Constrain to Template:**

```
"Answer using this format:
Answer: [your answer]
Evidence: [what supports this]
Uncertainty: [what you're unsure about]"

Structured output reduces creative fabrication
```

**Quick Comparison:**

```
Bad prompt: "Tell me about XYZ"
â†’ High hallucination risk

Good prompt: "Based on the context below, tell me about XYZ. 
If not in context, say 'information not available'."
â†’ Lower hallucination risk
```

**Interview Insight:** You can't eliminate hallucinations completely with prompting, but you can reduce them 50-70% with good techniques. Combine with RAG for production systems.

---

